<!DOCTYPE html>
<html lang="en" class="scroll-smooth">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Self-Evaluation Unlocks Any-Step Text to Image Generation</title>

  <!-- Release toggles -->
  <script>
    // Set to true when you have an official arXiv link / bibtex ready.
    const ENABLE_CITATION = false;
  </script>

  <!-- Fonts (paper-like serif for title/body) -->
  <link rel="preconnect" href="https://fonts.googleapis.com" />
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  <link
    href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&family=JetBrains+Mono:wght@400;500&family=Source+Serif+4:wght@400;600;700&display=swap"
    rel="stylesheet"
  />

  <!-- Tailwind -->
  <script src="https://cdn.tailwindcss.com"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          fontFamily: {
            serif: ['Source Serif 4', 'serif'],
            sans: ['Inter', 'sans-serif'],
            mono: ['JetBrains Mono', 'monospace'],
          },
          colors: {
            gray: {
              50: '#fafafa',
              100: '#f4f4f5',
              200: '#e4e4e7',
              300: '#d4d4d8',
              400: '#a1a1aa',
              500: '#71717a',
              600: '#52525b',
              700: '#3f3f46',
              800: '#27272a',
              900: '#18181b',
            },
            brand: {
              600: '#2563eb',
              100: '#dbeafe',
            },
          },
        },
      },
    };
  </script>

  <!-- MathJax -->
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
      },
      options: {
        ignoreHtmlClass: 'tex2jax_ignore',
        processHtmlClass: 'tex2jax_process',
      },
    };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <style>
    :root { --brand: #2563eb; }

    body {
      font-family: "Source Serif 4", serif;
      background: #ffffff;
      color: #18181b;
      -webkit-font-smoothing: antialiased;
      line-height: 1.7;
    }

    h1, h2, h3, h4 { letter-spacing: -0.02em; }
    p { margin-bottom: 1.15rem; }

    /* UI text (TOC labels etc.) */
    .ui-sans { font-family: "Inter", sans-serif; }

    .section-heading {
      font-family: "Inter", sans-serif;
      font-size: 0.75rem;
      font-weight: 600;
      text-transform: uppercase;
      letter-spacing: 0.08em;
      color: #64748b;
      margin-bottom: 0.85rem;
      display: block;
    }

    /* Sticky Sidebar TOC */
    .toc-link {
      font-family: "Inter", sans-serif;
      display: block;
      padding: 0.35rem 0;
      color: #a1a1aa;
      font-size: 0.875rem;
      font-weight: 400;
      transition: all 0.2s ease;
      border-left: 2px solid transparent;
      padding-left: 0.9rem;
      margin-left: -2px;
    }
    .toc-link:hover { color: #18181b; }
    .toc-link.active {
      color: #18181b;
      font-weight: 600;
      border-left-color: var(--brand);
    }

    a.link-muted { color: #52525b; transition: color 0.2s ease; }
    a.link-muted:hover { color: var(--brand); }

    figure { margin: 2rem 0; }
    figcaption {
      font-family: "Inter", sans-serif;
      font-size: 0.85rem;
      color: #71717a;
      margin-top: 0.75rem;
      text-align: left;
      font-weight: 400;
    }

    /* Legend tokens for green/blue/dashed blue */
    .legend {
      font-family: "Inter", sans-serif;
      display: inline-flex;
      align-items: center;
      gap: 0.35rem;
      white-space: nowrap;
      padding: 0.05rem 0.35rem;
      border-radius: 0.45rem;
      border: 1px solid rgba(0,0,0,0.06);
      vertical-align: baseline;
    }
    .legend-swatch {
      width: 0.72rem;
      height: 0.72rem;
      border-radius: 0.22rem;
      display: inline-block;
      flex: none;
    }
    .legend-green { color: #15803d; background: rgba(21,128,61,0.08); }
    .legend-green .legend-swatch { background: #16a34a; }

    .legend-blue { color: #1d4ed8; background: rgba(29,78,216,0.08); }
    .legend-blue .legend-swatch { background: #2563eb; }

    .legend-dashed-blue { color: #1d4ed8; background: rgba(29,78,216,0.06); }
    .legend-dashed-blue .legend-swatch {
      background: transparent;
      border: 2px dashed #2563eb;
      box-sizing: border-box;
    }
    .legend-dashed-blue .legend-text {
      text-decoration: underline;
      text-decoration-style: dashed;
      text-underline-offset: 3px;
    }

    /* Make tables more mobile friendly */
    .table-wrap { overflow-x: auto; -webkit-overflow-scrolling: touch; }
    table { font-family: "Inter", sans-serif; }

    /* Hide citation when disabled */
    .hidden-by-flag { display: none; }
  </style>
</head>

<body id="top" class="selection:bg-brand-100 selection:text-gray-900">
  <!-- Layout: center content, reduce empty space on desktop -->
  <div class="mx-auto max-w-[1220px] px-4 sm:px-6 lg:px-8 pt-12 lg:pt-16 lg:flex lg:gap-10">
    <!-- Left Sidebar (narrower) -->
    <aside class="hidden lg:block lg:w-56 ui-sans">
      <div class="sticky top-16">
        <div class="mb-8">
          <a href="#top" class="text-lg font-semibold tracking-tight text-gray-900 inline-flex items-center gap-2">
            <span class="w-2 h-2 rounded-full" style="background: var(--brand);"></span>
            <span>Self-E</span>
          </a>
        </div>

        <div class="mb-8">
          <div class="text-xs font-semibold text-gray-400 uppercase tracking-wider mb-3">Contents</div>
          <nav class="space-y-1" id="toc">
            <a href="#introduction" class="toc-link">Introduction</a>
            <a href="#method" class="toc-link">Method</a>
            <a href="#results" class="toc-link">Results</a>
            <a href="#intuition" class="toc-link">Matching to Evaluation</a>
            <a href="#conclusion" class="toc-link">Conclusion</a>
            <a href="#citation" class="toc-link" id="toc-citation">Citation</a>
          </nav>
        </div>
      </div>
    </aside>

    <!-- Main Content -->
    <main class="min-w-0 flex-1 pb-24 md:pb-32">
      <!-- Hero -->
      <header class="mb-14 md:mb-16">
        <h1 class="text-4xl md:text-5xl lg:text-6xl font-semibold text-gray-900 leading-[1.06] mb-8">
          Self-Evaluation Unlocks<br />
          <span class="text-gray-400">Any-Step Text-to-Image Generation</span>
        </h1>

        <!-- Authors/date compact -->
        <div class="grid grid-cols-1 md:grid-cols-12 gap-4 items-start">
          <div class="md:col-span-9 ui-sans">
            <div class="text-sm font-semibold text-gray-900">Authors</div>

            <div class="mt-2 flex flex-wrap gap-x-4 gap-y-1 text-sm text-gray-600 leading-relaxed">
              <span><a class="link-muted" href="https://xinyu-andy.github.io/" target="_blank" rel="noreferrer">Xin Yu</a><sup>1,2</sup></span>
              <span><a class="link-muted" href="https://scholar.google.com/citations?hl=en&user=bGn0uacAAAAJ&view_op=list_works&sortby=pubdate" target="_blank" rel="noreferrer">Xiaojuan Qi</a><sup>1*†</sup></span>
              <span><a class="link-muted" href="https://zhengqili.github.io/" target="_blank" rel="noreferrer">Zhengqi Li</a><sup>2</sup></span>
              <span><a class="link-muted" href="https://kai-46.github.io/website/" target="_blank" rel="noreferrer">Kai Zhang</a><sup>2</sup></span>
              <span><a class="link-muted" href="https://richzhang.github.io/" target="_blank" rel="noreferrer">Richard Zhang</a><sup>2</sup></span>
              <span><a class="link-muted" href="https://sites.google.com/site/zhelin625/" target="_blank" rel="noreferrer">Zhe Lin</a><sup>2</sup></span>
              <span><a class="link-muted" href="https://scholar.google.com/citations?user=B_FTboQAAAAJ&hl=zh-CN" target="_blank" rel="noreferrer">Eli Shechtman</a><sup>2</sup></span>
              <span><a class="link-muted" href="https://stevewongv.github.io/" target="_blank" rel="noreferrer">Tianyu Wang</a><sup>2†</sup></span>
              <span><a class="link-muted" href="https://yotamnitzan.github.io/" target="_blank" rel="noreferrer">Yotam Nitzan</a><sup>2†</sup></span>
            </div>

            <div class="mt-2 text-xs text-gray-500 leading-relaxed">
              <div><sup>1</sup>The University of Hong Kong &nbsp;&nbsp; <sup>2</sup>Adobe Research</div>
              <div><sup>*</sup>Project lead. &nbsp;&nbsp; <sup>†</sup>Corresponding authors.</div>
            </div>
          </div>

          <div class="md:col-span-3 md:text-right ui-sans">
            <div class="text-sm font-semibold text-gray-900">December 2025</div>
          </div>
        </div>

        <!-- Teaser -->
        <div class="mt-10 md:mt-12">
          <figure>
            <div class="rounded-lg bg-gray-50 border border-gray-100 overflow-hidden shadow-sm">
              <img src="teaser.svg" alt="Teaser" class="w-full h-auto" loading="eager" />
            </div>
            <figcaption>
              Figure 1. One model, any compute: Self-E generates coherent images at 2, 4, 8, and 50 steps.
            </figcaption>
          </figure>
        </div>
      </header>

      <!-- Introduction -->
      <section id="introduction" class="mb-18 md:mb-20 scroll-mt-20">
        <span class="section-heading">Introduction</span>
        <div class="text-gray-800 text-lg leading-relaxed">
          <p>
            Modern text to image models are dominated by diffusion and flow matching due to their stability, scalability, and strong visual fidelity.
            However, they are inherently multi-step models: they learn local scores or velocities and therefore require dozens of steps to reliably traverse a curved reverse trajectory.
          </p>
          <p>
            We introduce the <span class="font-semibold">Self-Evaluating Model (Self-E)</span>, a <span class="font-semibold">from scratch</span> training method for any-step text to image generation without distillation from a pretrained teacher.
            Self-E learns from data similarly to flow matching, while simultaneously employing a self-evaluation mechanism that evaluates its own generated samples using current score estimates, effectively serving as a dynamic self-teacher.
            This complements local learning from data with an explicit notion of evaluation at the landing point: the model generates a candidate jump and then critiques and refines it using a learned score estimate at the landing point.
          </p>
        </div>
      </section>

      <hr class="border-gray-100 my-16 md:my-18 w-full" />

      <!-- Method -->
      <section id="method" class="mb-20 md:mb-24 scroll-mt-20">
        <span class="section-heading">Method</span>
        <h2 class="text-3xl md:text-4xl font-semibold text-gray-900 mb-6">Two Complementary Signals</h2>

        <p class="text-lg text-gray-700 max-w-3xl mb-10">
          Self-E trains a single model with two complementary objectives: a learning from data component that provides local trajectory supervision, and a self-evaluation component that targets global distribution matching.
        </p>

        <figure>
          <div class="rounded-lg bg-gray-50 border border-gray-100 overflow-hidden shadow-sm p-4 md:p-6">
            <img src="method.svg" class="w-full rounded-lg" alt="Self-E method overview" loading="lazy" />
          </div>
          <figcaption>
            Self-E simultaneously learns from data while performing self-evaluation, using the same network in two complementary modes.
          </figcaption>
        </figure>

        <div class="mt-10 space-y-6">
          <div class="bg-white p-7 rounded-2xl border border-gray-100 shadow-sm">
            <div class="ui-sans text-xs font-semibold text-gray-500 uppercase tracking-wide mb-2">Learning from data</div>
            <p class="text-gray-700 text-base leading-relaxed mb-0">
              <span class="font-semibold">What it learns:</span> local structure, i.e., the local score or velocity information that explains how density varies in nearby states.
              Concretely, we sample a real image $x_0$ with prompt $c$, add noise to obtain $x_t$, and train the model to predict the clean image from this noisy input using a conditional flow matching objective.
              This provides local trajectory supervision: it teaches reliable local behavior and is naturally most effective when generation follows a local path with many small steps.
            </p>
          </div>

          <div class="bg-white p-7 rounded-2xl border border-gray-100 shadow-sm">
            <div class="ui-sans text-xs font-semibold text-gray-500 uppercase tracking-wide mb-2">Learning by self-evaluation</div>
            <p class="text-gray-700 text-base leading-relaxed mb-0">
              <span class="font-semibold">What it learns:</span> global correctness of the generated sample, i.e., whether a landed output is realistic and prompt-consistent.
              Instead of constraining the intermediate generation path, self-evaluation directly targets global distribution matching by treating the model output as a sample from its implicit distribution and pushing it toward the real data distribution.
              After the model proposes a long-range jump, it uses its own local estimator at the landing point to produce a direction signal that indicates how the current sample should move toward a better, more prompt-consistent region.
              In most of our training, this direction is the classifier free signal computed from conditional and unconditional predictions, which we find stable and effective for improving text to image alignment.
              Later, we incorporate an additional fake score signal that more directly supports distribution matching.
            </p>
          </div>

          <div class="bg-white p-7 rounded-2xl border border-gray-100 shadow-sm">
            <div class="ui-sans text-xs font-semibold text-gray-500 uppercase tracking-wide mb-2">Closed loop</div>
            <p class="text-gray-700 text-base leading-relaxed mb-6">
              Conceptually, this can be viewed through an environment-agent lens.
              The environment corresponds to the local estimates learned from real data during training, while the agent is the generator used at inference time.
              The loop closes when local estimates are reused to evaluate landing points and improve long-range jumps.
            </p>

            <div class="border-l border-gray-200 pl-8 space-y-7">
              <div class="relative">
                <span class="ui-sans absolute -left-[39px] top-0.5 w-5 h-5 rounded-full border border-gray-300 bg-white text-[10px] text-gray-500 flex items-center justify-center font-mono">1</span>
                <h4 class="text-lg font-semibold text-gray-900 mb-1">Data Phase</h4>
                <p class="text-base text-gray-700 mb-0">
                  The model learns local structure from real samples $(x_0, c)$ and their noisy states $x_t$, producing an evolving local score or velocity signal around noisy inputs.
                  This yields an internal evaluator.
                </p>
              </div>

              <div class="relative">
                <span class="ui-sans absolute -left-[39px] top-0.5 w-5 h-5 rounded-full border border-gray-300 bg-white text-[10px] text-gray-500 flex items-center justify-center font-mono">2</span>
                <h4 class="text-lg font-semibold text-gray-900 mb-1">Self-Evaluation Phase</h4>
                <p class="text-base text-gray-700 mb-0">
                  The model proposes a long-range jump and then performs sample evaluation to assess where it lands.
                  This trains the generator to land in higher-density, prompt-consistent regions.
                </p>
              </div>

              <div class="relative">
                <span class="ui-sans absolute -left-[39px] top-0.5 w-5 h-5 rounded-full border border-gray-900 bg-gray-900 text-[10px] text-white flex items-center justify-center font-mono">3</span>
                <h4 class="text-lg font-semibold text-gray-900 mb-1">Closed Loop</h4>
                <p class="text-base text-gray-700 mb-0">
                  Better learning from data improves the evaluator.
                  A better evaluator improves few-step behavior.
                  These components reinforce each other throughout training without pretrained teacher distillation.
                </p>
              </div>
            </div>
          </div>
        </div>
      </section>

      <!-- Results -->
      <section id="results" class="mb-20 md:mb-24 scroll-mt-20">
        <span class="section-heading">Results</span>
        <h2 class="text-3xl md:text-4xl font-semibold text-gray-900 mb-5">GenEval Overall Across Step Counts</h2>

        <p class="text-gray-700 text-base leading-relaxed max-w-4xl mb-10">
          Self-E is consistently state of the art across step budgets and improves monotonically with more steps.
          2, 4, 8, and 50 steps correspond to 0.753, 0.781, 0.785, and 0.815.
          The largest margin appears in the few-step regime, while performance remains top-tier at 8 and 50 steps.
        </p>

        <div class="grid lg:grid-cols-2 gap-8">
          <!-- Table -->
          <div class="bg-white rounded-xl border border-gray-100 shadow-sm overflow-hidden">
            <div class="ui-sans px-6 py-4 border-b border-gray-100">
              <h3 class="text-base font-semibold text-gray-900 mb-1">Quantitative Comparison</h3>
              <div class="text-xs text-gray-500">Metric: GenEval Overall</div>
            </div>

            <div class="table-wrap">
              <table class="w-full text-sm text-left border-collapse">
                <thead class="text-gray-500 bg-gray-50 border-b border-gray-200">
                  <tr>
                    <th class="py-3 px-4 font-semibold">Method</th>
                    <th class="py-3 px-4 font-semibold">2</th>
                    <th class="py-3 px-4 font-semibold">4</th>
                    <th class="py-3 px-4 font-semibold">8</th>
                    <th class="py-3 px-4 font-semibold">50</th>
                  </tr>
                </thead>
                <tbody class="text-gray-700 divide-y divide-gray-100 bg-white">
                  <tr>
                    <td class="py-3 px-4">SDXL</td>
                    <td class="py-3 px-4 text-gray-600">0.0021</td>
                    <td class="py-3 px-4 text-gray-600">0.1576</td>
                    <td class="py-3 px-4 text-gray-600">0.3759</td>
                    <td class="py-3 px-4 text-gray-600">0.4601</td>
                  </tr>
                  <tr>
                    <td class="py-3 px-4">FLUX.1-Dev</td>
                    <td class="py-3 px-4 text-gray-600">0.0998</td>
                    <td class="py-3 px-4 text-gray-600">0.3198</td>
                    <td class="py-3 px-4 text-gray-600">0.5893</td>
                    <td class="py-3 px-4 text-gray-600">0.7966</td>
                  </tr>
                  <tr>
                    <td class="py-3 px-4">LCM</td>
                    <td class="py-3 px-4 text-gray-600">0.2624</td>
                    <td class="py-3 px-4 text-gray-600">0.3277</td>
                    <td class="py-3 px-4 text-gray-600">0.3398</td>
                    <td class="py-3 px-4 text-gray-600">0.3303</td>
                  </tr>
                  <tr>
                    <td class="py-3 px-4">SANA-1.5</td>
                    <td class="py-3 px-4 text-gray-600">0.1662</td>
                    <td class="py-3 px-4 text-gray-600">0.5725</td>
                    <td class="py-3 px-4 text-gray-600">0.7788</td>
                    <td class="py-3 px-4 text-gray-600">0.8062</td>
                  </tr>
                  <tr>
                    <td class="py-3 px-4">TiM</td>
                    <td class="py-3 px-4 text-gray-600">0.6338</td>
                    <td class="py-3 px-4 text-gray-600">0.6867</td>
                    <td class="py-3 px-4 text-gray-600">0.7143</td>
                    <td class="py-3 px-4 text-gray-600">0.7797</td>
                  </tr>
                  <tr>
                    <td class="py-3 px-4">SDXL-Turbo</td>
                    <td class="py-3 px-4 text-gray-600">0.4622</td>
                    <td class="py-3 px-4 text-gray-600">0.4766</td>
                    <td class="py-3 px-4 text-gray-600">0.4652</td>
                    <td class="py-3 px-4 text-gray-600">0.3983</td>
                  </tr>
                  <tr>
                    <td class="py-3 px-4">SD3.5-Turbo</td>
                    <td class="py-3 px-4 text-gray-600">0.3635</td>
                    <td class="py-3 px-4 text-gray-600">0.7194</td>
                    <td class="py-3 px-4 text-gray-600">0.7071</td>
                    <td class="py-3 px-4 text-gray-600">0.6114</td>
                  </tr>
                  <tr class="bg-brand-100/25 border-l-4" style="border-left-color: var(--brand);">
                    <td class="py-3 px-4">Self-E</td>
                    <td class="py-3 px-4 text-gray-900">0.7531</td>
                    <td class="py-3 px-4 text-gray-900">0.7806</td>
                    <td class="py-3 px-4 text-gray-900">0.7849</td>
                    <td class="py-3 px-4 text-gray-900">0.8151</td>
                  </tr>
                </tbody>
              </table>
            </div>
          </div>

          <!-- Chart -->
          <div class="bg-white rounded-xl border border-gray-100 shadow-sm overflow-hidden">
            <div class="ui-sans px-6 py-4 border-b border-gray-100">
              <h3 class="text-base font-semibold text-gray-900 mb-1">Overall vs Steps</h3>
              <div class="text-xs text-gray-500">x-axis: step count, y-axis: GenEval Overall</div>
            </div>

            <div class="p-6">
              <svg id="genevalOverallChart" viewBox="0 0 520 280" class="w-full h-[280px]" role="img" aria-label="GenEval Overall vs steps"></svg>

              <div class="ui-sans mt-4 flex flex-wrap gap-x-4 gap-y-2 text-xs text-gray-600">
                <span class="inline-flex items-center gap-2"><span class="w-2 h-2 rounded-full" style="background:#a1a1aa"></span>SDXL</span>
                <span class="inline-flex items-center gap-2"><span class="w-2 h-2 rounded-full" style="background:#14b8a6"></span>FLUX.1-Dev</span>
                <span class="inline-flex items-center gap-2"><span class="w-2 h-2 rounded-full" style="background:#64748b"></span>LCM</span>
                <span class="inline-flex items-center gap-2"><span class="w-2 h-2 rounded-full" style="background:#0ea5e9"></span>SANA-1.5</span>
                <span class="inline-flex items-center gap-2"><span class="w-2 h-2 rounded-full" style="background:#6366f1"></span>TiM</span>
                <span class="inline-flex items-center gap-2"><span class="w-2 h-2 rounded-full" style="background:#06b6d4"></span>SDXL-Turbo</span>
                <span class="inline-flex items-center gap-2"><span class="w-2 h-2 rounded-full" style="background:#8b5cf6"></span>SD3.5-Turbo</span>
                <span class="inline-flex items-center gap-2"><span class="w-2 h-2 rounded-full" style="background:var(--brand)"></span>Self-E</span>
              </div>

              <script>
                (() => {
                  const svg = document.getElementById('genevalOverallChart');
                  if (!svg) return;

                  const NS = 'http://www.w3.org/2000/svg';
                  const width = 520;
                  const height = 280;
                  const pad = { l: 56, r: 18, t: 18, b: 44 };

                  const steps = [2, 4, 8, 50];
                  const yMin = 0.0;
                  const yMax = 0.85;

                  const brand = getComputedStyle(document.documentElement).getPropertyValue('--brand').trim() || '#2563eb';
                  const series = [
                    { name: 'SDXL', color: '#a1a1aa', values: [0.0021, 0.1576, 0.3759, 0.4601] },
                    { name: 'FLUX.1-Dev', color: '#14b8a6', values: [0.0998, 0.3198, 0.5893, 0.7966] },
                    { name: 'LCM', color: '#64748b', values: [0.2624, 0.3277, 0.3398, 0.3303] },
                    { name: 'SANA-1.5', color: '#0ea5e9', values: [0.1662, 0.5725, 0.7788, 0.8062] },
                    { name: 'TiM', color: '#6366f1', values: [0.6338, 0.6867, 0.7143, 0.7797] },
                    { name: 'SDXL-Turbo', color: '#06b6d4', values: [0.4622, 0.4766, 0.4652, 0.3983] },
                    { name: 'SD3.5-Turbo', color: '#8b5cf6', values: [0.3635, 0.7194, 0.7071, 0.6114] },
                    { name: 'Self-E', color: brand, values: [0.7531, 0.7806, 0.7849, 0.8151], width: 3.2 },
                  ];

                  const x = (i) => pad.l + (i * (width - pad.l - pad.r)) / (steps.length - 1);
                  const y = (v) => {
                    const t = (v - yMin) / (yMax - yMin);
                    return pad.t + (1 - t) * (height - pad.t - pad.b);
                  };

                  const add = (tag, attrs) => {
                    const el = document.createElementNS(NS, tag);
                    Object.entries(attrs).forEach(([k, v]) => el.setAttribute(k, String(v)));
                    svg.appendChild(el);
                    return el;
                  };

                  while (svg.firstChild) svg.removeChild(svg.firstChild);

                  const yTicks = [0.0, 0.2, 0.4, 0.6, 0.8];
                  yTicks.forEach((t) => {
                    const yy = y(t);
                    add('line', { x1: pad.l, y1: yy, x2: width - pad.r, y2: yy, stroke: '#e4e4e7', 'stroke-width': 1 });
                    const txt = add('text', {
                      x: pad.l - 10,
                      y: yy + 4,
                      'text-anchor': 'end',
                      fill: '#71717a',
                      'font-size': 10,
                      'font-family': 'Inter, sans-serif',
                    });
                    txt.textContent = t.toFixed(1);
                  });

                  add('line', { x1: pad.l, y1: pad.t, x2: pad.l, y2: height - pad.b, stroke: '#d4d4d8', 'stroke-width': 1 });
                  add('line', { x1: pad.l, y1: height - pad.b, x2: width - pad.r, y2: height - pad.b, stroke: '#d4d4d8', 'stroke-width': 1 });

                  steps.forEach((s, i) => {
                    const xx = x(i);
                    add('line', { x1: xx, y1: height - pad.b, x2: xx, y2: height - pad.b + 4, stroke: '#d4d4d8', 'stroke-width': 1 });
                    const txt = add('text', {
                      x: xx,
                      y: height - pad.b + 18,
                      'text-anchor': 'middle',
                      fill: '#71717a',
                      'font-size': 10,
                      'font-family': 'Inter, sans-serif',
                    });
                    txt.textContent = String(s);
                  });

                  series.forEach((s) => {
                    const d = s.values.map((v, i) => `${i === 0 ? 'M' : 'L'} ${x(i).toFixed(2)} ${y(v).toFixed(2)}`).join(' ');
                    add('path', {
                      d,
                      fill: 'none',
                      stroke: s.color,
                      'stroke-width': s.width || 2,
                      'stroke-linecap': 'round',
                      'stroke-linejoin': 'round',
                      opacity: s.name === 'Self-E' ? 1 : 0.85,
                    });

                    s.values.forEach((v, i) => {
                      add('circle', {
                        cx: x(i),
                        cy: y(v),
                        r: s.name === 'Self-E' ? 3.2 : 2.2,
                        fill: s.color,
                        stroke: '#ffffff',
                        'stroke-width': 1,
                        opacity: s.name === 'Self-E' ? 1 : 0.9,
                      });
                    });
                  });

                  const ylab = add('text', {
                    x: pad.l,
                    y: pad.t - 6,
                    'text-anchor': 'start',
                    fill: '#71717a',
                    'font-size': 10,
                    'font-family': 'Inter, sans-serif',
                  });
                  ylab.textContent = 'Overall';

                  const xlab = add('text', {
                    x: width - pad.r,
                    y: height - 10,
                    'text-anchor': 'end',
                    fill: '#71717a',
                    'font-size': 10,
                    'font-family': 'Inter, sans-serif',
                  });
                  xlab.textContent = 'Steps';
                })();
              </script>
            </div>
          </div>
        </div>

        <figure class="mt-12">
          <div class="rounded-lg bg-gray-50 border border-gray-100 overflow-hidden shadow-sm">
            <img src="comparison.svg" alt="Qualitative comparison" class="w-full h-auto" loading="lazy" />
          </div>
          <figcaption>
            Qualitative comparison. Side-by-side visual results across different step budgets.
          </figcaption>
        </figure>
      </section>

      <!-- Matching to Evaluation -->
      <section id="intuition" class="mb-20 md:mb-24 scroll-mt-20">
        <span class="section-heading">Matching to Evaluation</span>
        <h2 class="text-3xl md:text-4xl font-semibold text-gray-900 mb-6">The Conceptual Shift</h2>

        <p class="text-lg text-gray-700 max-w-3xl mb-10">
          For a fixed noisy input, training can be viewed as learning directions on an energy landscape.
          In the animations, <span class="legend legend-green"><span class="legend-swatch"></span><span class="legend-text">green</span></span> corresponds to a score-driven better direction,
          <span class="legend legend-blue"><span class="legend-swatch"></span><span class="legend-text">blue</span></span> corresponds to the model prediction, and
          <span class="legend legend-dashed-blue"><span class="legend-swatch"></span><span class="legend-text">dashed blue</span></span> corresponds to the supervision signal used to update the model.
          The key shift is where supervision is applied: match a local direction at the start, or evaluate the quality of the landing point.
        </p>

        <!-- Diffusion -->
        <div class="mb-14">
          <div class="ui-sans text-xs font-semibold text-gray-500 uppercase tracking-wide mb-2">Matching at the start point</div>
          <h3 class="text-2xl font-semibold text-gray-900 mb-4">Diffusion</h3>

          <div class="grid grid-cols-1 md:grid-cols-12 gap-6 md:gap-8 items-start">
            <div class="md:col-span-7">
              <p class="text-gray-700 text-base">
                Diffusion provides a static target: for a given noisy input, its score function defines the ground-truth local direction.
                Training is standard supervised learning: update the model so its prediction aligns with the target.
              </p>
              <p class="text-gray-700 text-base">
                Even with perfect local matching, inference still needs many steps.
                Starting from noise, the sampler must integrate these local directions step by step to follow a curved trajectory toward higher-density regions.
                This is why scaling model size alone cannot remove the step bottleneck: the limitation is geometry and numerical integration.
              </p>
            </div>

            <div class="md:col-span-5">
              <div class="rounded-lg bg-gray-50 border border-gray-100 p-4">
                <img src="gif1.gif" class="w-full h-auto max-h-[260px] object-contain rounded bg-white border border-gray-100" alt="Diffusion training animation" loading="lazy" />
                <p class="ui-sans mt-3 text-xs text-gray-500 mb-0">
                  Animation over training iterations: the model is updated so its prediction aligns with the fixed local target; the dashed arrow indicates the update direction.
                </p>
              </div>
            </div>

            <!-- Force full-row aligned blocks -->
            <div class="md:col-span-12">
              <div class="mt-1 rounded-2xl border border-gray-100 bg-gray-50/60 p-4">
                <div class="grid gap-3 md:grid-cols-3">
                  <div class="rounded-xl bg-white border border-gray-100 p-4">
                    <div class="ui-sans text-[11px] font-semibold text-gray-500 uppercase tracking-wide">Supervision</div>
                    <div class="mt-2 text-sm text-gray-700 leading-relaxed">Fixed local target at the start point.</div>
                  </div>
                  <div class="rounded-xl bg-white border border-gray-100 p-4">
                    <div class="ui-sans text-[11px] font-semibold text-gray-500 uppercase tracking-wide">What it learns</div>
                    <div class="mt-2 text-sm text-gray-700 leading-relaxed">A local vector field that supports many-step integration, not a shortcut.</div>
                  </div>
                  <div class="rounded-xl bg-white border border-gray-100 p-4">
                    <div class="ui-sans text-[11px] font-semibold text-gray-500 uppercase tracking-wide">Consequence</div>
                    <div class="mt-2 text-sm text-gray-700 leading-relaxed">Few large steps extrapolate and tend to drift toward average behavior.</div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>

        <!-- Self-E -->
        <div>
          <div class="ui-sans text-xs font-semibold text-gray-500 uppercase tracking-wide mb-2">Evaluation at the landing point</div>
          <h3 class="text-2xl font-semibold text-gray-900 mb-4">Self-E</h3>

          <div class="grid grid-cols-1 md:grid-cols-12 gap-6 md:gap-8 items-start">
            <div class="md:col-span-7">
              <p class="text-gray-700 text-base">
                Self-E changes the training target from matching a direction to reaching a good destination.
                At each iteration, the model proposes a long-range jump to a landing candidate.
                The landed point is evaluated: the local direction at the landing point indicates how to move toward a better, higher-density region.
                This produces a dynamic supervision signal that teaches the model to directly aim for better destinations.
              </p>
              <p class="text-gray-700 text-base">
                In other words, the model proposes, the proposal is evaluated, and learning happens from feedback.
                This outcome-oriented supervision implicitly shapes a reliable shortcut path.
                Self-E training resembles a refinement step used during diffusion inference, while at inference time Self-E can output the shortcut directly.
              </p>
            </div>

            <div class="md:col-span-5">
              <div class="rounded-lg bg-gray-50 border border-gray-100 p-4">
                <img src="gif2.gif" class="w-full h-auto max-h-[260px] object-contain rounded bg-white border border-gray-100" alt="Self-E training animation" loading="lazy" />
                <p class="ui-sans mt-3 text-xs text-gray-500 mb-0">
                  Animation over training iterations: the model proposes a long-range jump, is evaluated at the landed point, and is updated using a feedback direction toward a better target.
                </p>
              </div>
            </div>

            <div class="md:col-span-12">
              <div class="mt-1 rounded-2xl border border-gray-100 bg-gray-50/60 p-4">
                <div class="grid gap-3 md:grid-cols-3">
                  <div class="rounded-xl bg-white border border-gray-100 p-4">
                    <div class="ui-sans text-[11px] font-semibold text-gray-500 uppercase tracking-wide">Supervision</div>
                    <div class="mt-2 text-sm text-gray-700 leading-relaxed">Feedback at the landing point, dynamic rather than fixed at the start.</div>
                  </div>
                  <div class="rounded-xl bg-white border border-gray-100 p-4">
                    <div class="ui-sans text-[11px] font-semibold text-gray-500 uppercase tracking-wide">What it learns</div>
                    <div class="mt-2 text-sm text-gray-700 leading-relaxed">How to land in good enough regions in few steps.</div>
                  </div>
                  <div class="rounded-xl bg-white border border-gray-100 p-4">
                    <div class="ui-sans text-[11px] font-semibold text-gray-500 uppercase tracking-wide">Consequence</div>
                    <div class="mt-2 text-sm text-gray-700 leading-relaxed">The model implicitly learns a shortcut path.</div>
                  </div>
                </div>
              </div>
            </div>

            <!-- Self-E sub-block -->
            <div class="md:col-span-12">
              <div class="mt-4 rounded-2xl border border-gray-100 bg-white shadow-sm p-6">
                <div class="ui-sans text-xs font-semibold text-gray-500 uppercase tracking-wide mb-2">Where does the evaluation signal come from</div>
                <h4 class="text-xl font-semibold text-gray-900 mb-2">Evaluate by itself</h4>
                <p class="text-base text-gray-700 leading-relaxed mb-0">
                  Evaluating a landing point requires a score-like signal to indicate whether the proposed destination is good, but this signal is not directly available.
                  Prior work typically obtains it from a pretrained diffusion teacher.
                  Self-E instead co-trains the evaluator via learning from data and reuses it to provide feedback to the generator.
                  This enables a fully from scratch training setup without relying on any external model.
                </p>
              </div>
            </div>
          </div>
        </div>
      </section>

      <!-- Conclusion -->
      <section id="conclusion" class="mb-20 md:mb-24 scroll-mt-20">
        <span class="section-heading">Conclusion</span>
        <h2 class="text-3xl md:text-4xl font-semibold text-gray-900 mb-6">Future Work</h2>

        <p class="text-gray-700 text-base leading-relaxed max-w-4xl">
          Self-E introduces a pretraining paradigm that differs from trajectory-based training that mainly matches local directions along a path.
          By jointly learning local estimators from data and using them to supervise long-range jumps, Self-E enables flexible any-step inference without pretrained teacher distillation.
        </p>

        <p class="text-gray-700 text-base leading-relaxed max-w-4xl">
          The current approach is still at an early stage.
          In extremely low step regimes, generated images can miss fine details compared with long multi-step inference.
          Several design choices remain underexplored, including objective weighting, inference-time scheduling, and its adapation for downsteam tasks.
          We expect systematic optimization of these factors to yield further gains.
        </p>
      </section>

      <!-- Citation (hidden by default until ENABLE_CITATION=true) -->
      <section id="citation" class="mb-10 pt-12 border-t border-gray-100 scroll-mt-20 hidden-by-flag">
        <span class="section-heading">Citation</span>
        <div class="bg-gray-50 rounded-xl p-6 border border-gray-100 ui-sans">
          <div class="text-sm text-gray-700 mb-3">BibTeX</div>
          <div class="relative group">
            <button
              type="button"
              onclick="navigator.clipboard.writeText(document.getElementById('bibtex').innerText); this.innerText='Copied';"
              class="absolute top-3 right-3 bg-white border border-gray-200 px-2 py-1 rounded text-[10px] text-gray-500 opacity-0 group-hover:opacity-100 transition-opacity"
            >
              Copy
            </button>
<pre id="bibtex" class="font-mono text-xs text-gray-600 overflow-x-auto">@article{yu2025selfe,
  title={Self-Evaluation Unlocks Any-Step Text to Image Generation},
  author={Yu, Xin and Qi, Xiaojuan and Li, Zhengqi and Zhang, Kai and Zhang, Richard and Lin, Zhe and Shechtman, Eli and Wang, Tianyu and Nitzan, Yotam},
  journal={arXiv preprint},
  year={2025}
}</pre>
          </div>
        </div>
      </section>

      <footer class="py-8 ui-sans text-gray-400 text-sm">
        <p class="mb-0">&copy; 2025 Self-E Project.</p>
      </footer>
    </main>
  </div>

  <!-- Enable/disable citation + TOC entry -->
  <script>
    document.addEventListener('DOMContentLoaded', () => {
      // Citation feature flag
      const citationSection = document.getElementById('citation');
      const citationToc = document.getElementById('toc-citation');

      if (ENABLE_CITATION) {
        if (citationSection) citationSection.classList.remove('hidden-by-flag');
        if (citationToc) citationToc.style.display = '';
      } else {
        if (citationSection) citationSection.classList.add('hidden-by-flag');
        if (citationToc) citationToc.style.display = 'none';
      }

      // TOC active state
      const sections = document.querySelectorAll('section[id]');
      const navLinks = document.querySelectorAll('.toc-link');

      const onScroll = () => {
        let current = '';
        const offset = 140;

        sections.forEach((section) => {
          const top = section.offsetTop;
          if (window.scrollY >= top - offset) current = section.getAttribute('id');
        });

        navLinks.forEach((link) => {
          link.classList.remove('active');
          const href = link.getAttribute('href') || '';
          if (current && href === `#${current}`) link.classList.add('active');
        });
      };

      window.addEventListener('scroll', onScroll);
      onScroll();
    });
  </script>
</body>
</html>
